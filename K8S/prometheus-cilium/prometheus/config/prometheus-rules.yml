groups:
  - name: "Prometheus self-monitoring"
    rules:
      - alert: "Prometheus TSDB checkpoint creation failed"
        expr: 'increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus TSDB checkpoint creation failed (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} checkpoint creation failures.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus TSDB checkpoint deletion failed"
        expr: 'increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus TSDB checkpoint deletion failed (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} checkpoint deletion failures.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus TSDB compactions failed"
        expr: 'increase(prometheus_tsdb_compactions_failed_total[1m]) > 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus TSDB compactions failed (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB compactions failures.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus TSDB head truncations failed"
        expr: 'increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus TSDB head truncations failed (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB head truncation failures.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus TSDB reload failures"
        expr: 'increase(prometheus_tsdb_reloads_failures_total[1m]) > 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus TSDB reload failures (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB reload failures.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus TSDB WAL corruptions"
        expr: 'increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB WAL corruptions.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus TSDB WAL truncations failed"
        expr: 'increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})"
          description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus target missing"
        expr: "up == 0"
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus target missing (instance {{ $labels.instance }})"
          description: "A Prometheus target has disappeared. An exporter might be crashed.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus target empty"
        expr: 'prometheus_sd_discovered_targets == 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus target empty (instance {{ $labels.instance }})"
          description: "Prometheus has no target in service discovery.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus target scrape duplicate"
        expr: 'increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0'
        for: "1m"
        labels:
          severity: "warning"
        annotations:
          summary: "Prometheus target scrape duplicate (instance {{ $labels.instance }})"
          description: "Prometheus has many samples rejected due to duplicate timestamps but different values.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus configuration reload failure"
        expr: 'prometheus_config_last_reload_successful == 0'
        for: "1m"
        labels:
          severity: "warning"
        annotations:
          summary: "Prometheus configuration reload failure (instance {{ $labels.instance }})"
          description: "Prometheus configuration reload error.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus not connected to Alertmanager"
        expr: 'prometheus_notifications_alertmanagers_discovered < 1'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus not connected to Alertmanager (instance {{ $labels.instance }})"
          description: "Prometheus cannot connect the alertmanager.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Alertmanager notification failure"
        expr: 'rate(alertmanager_notifications_failed_total[1m]) > 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus AlertManager notification failure (instance {{ $labels.instance }})"
          description: "Alertmanager is failure sending notifications.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Alertmanager configuration reload failure"
        expr: 'alertmanager_config_last_reload_successful == 0'
        for: "1m"
        labels:
          severity: "warning"
        annotations:
          summary: "Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})"
          description: "AlertManager configuration reload error.\nValue: {{ $value }}\nLabels: {{ $labels }}"


  - name: "Blackbox service check"
    rules:
      - alert: "Blackbox slow probe"
        expr: 'avg_over_time(probe_duration_seconds[1m]) > 1'
        for: "1m"
        labels:
          severity: "warning"
        annotations:
          summary: "Blackbox slow probe (instance {{ $labels.instance }})"
          description: "Blackbox probe took more than 1s to complete.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Blackbox probe failure"
        expr: 'probe_http_status_code < 100 or probe_http_status_code > 299'
        for: "1m"
        labels:
          severity: "error"
        annotations:
          summary: "Blackbox probe HTTP failure (instance {{ $labels.instance }})"
          description: "HTTP status code is not 100-299.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Blackbox SSL certificate will expire soon"
        expr: '(probe_ssl_earliest_cert_expiry - time()) < (86400 * 15)'
        for: "1m"
        labels:
          severity: "warning"
        annotations:
          summary: "Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})"
          description: "SSL certificate expires in 15 days.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Blackbox SSL certificate will expire soon"
        expr: '(probe_ssl_earliest_cert_expiry - time()) < (86400 * 7)'
        for: "1m"
        labels:
          severity: "error"
        annotations:
          summary: "Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})"
          description: "SSL certificate expires in 7 days.\nValue: {{ $value }}\nLabels: {{ $labels }}"
