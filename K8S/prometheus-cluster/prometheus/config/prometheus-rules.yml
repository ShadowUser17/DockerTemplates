groups:
  - name: "Prometheus self-monitoring"
    rules:
      - alert: "Prometheus target missing"
        expr: "up == 0"
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus target missing (instance {{ $labels.instance }})"
          description: "A Prometheus target has disappeared. An exporter might be crashed.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus target empty"
        expr: 'prometheus_sd_discovered_targets == 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus target empty (instance {{ $labels.instance }})"
          description: "Prometheus has no target in service discovery.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus target scrape duplicate"
        expr: 'increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0'
        for: "1m"
        labels:
          severity: "warning"
        annotations:
          summary: "Prometheus target scrape duplicate (instance {{ $labels.instance }})"
          description: "Prometheus has many samples rejected due to duplicate timestamps but different values.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Prometheus not connected to Alertmanager"
        expr: 'prometheus_notifications_alertmanagers_discovered < 1'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus not connected to Alertmanager (instance {{ $labels.instance }})"
          description: "Prometheus cannot connect the alertmanager.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Alertmanager notification failing"
        expr: 'rate(alertmanager_notifications_failed_total[1m]) > 0'
        for: "1m"
        labels:
          severity: "critical"
        annotations:
          summary: "Prometheus AlertManager notification failing (instance {{ $labels.instance }})"
          description: "Alertmanager is failing sending notifications.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Alertmanager configuration reload failure"
        expr: 'alertmanager_config_last_reload_successful != 1'
        for: "1m"
        labels:
          severity: "warning"
        annotations:
          summary: "Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})"
          description: "AlertManager configuration reload error.\nValue: {{ $value }}\nLabels: {{ $labels }}"


  - name: "Blackbox service check"
    rules:
      - alert: "Blackbox slow probe"
        expr: 'avg_over_time(probe_duration_seconds[1m]) > 1'
        for: "1m"
        labels:
          severity: "warning"
        annotations:
          summary: "Blackbox slow probe (instance {{ $labels.instance }})"
          description: "Blackbox probe took more than 1s to complete.\nValue: {{ $value }}\nLabels: {{ $labels }}"

      - alert: "Blackbox probe failure"
        expr: 'probe_http_status_code < 200 or probe_http_status_code > 399'
        for: "1m"
        labels:
          severity: "error"
        annotations:
          summary: "Blackbox probe HTTP failure (instance {{ $labels.instance }})"
          description: "HTTP status code is not 200-399.\nValue: {{ $value }}\nLabels: {{ $labels }}"
